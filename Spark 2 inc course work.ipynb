{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\devenv\\\\SPARK\\\\spark-3.0.0-preview2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This from the course\n",
    "spark = SparkSession.builder.appName('Ops').getOrCreate() # .builder not .Builder !! waht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is mashed together from the book + a medium article\n",
    "# If I run this after running the above it will error\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# findspark.find()\n",
    "# import pyspark\n",
    "# findspark.find()\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# conf = SparkConf().setAppName('Ops').setMaster('local')\n",
    "# sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Shared Files\\Udemy Files\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shared Files\\\\Udemy Files\\\\Python-and-Spark-for-Big-Data-master\\\\Spark_DataFrames'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = spark.read.json('people.json') # this errors - spark does not seem to care about chdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note need to escape the final \\ even in a raw string\n",
    "file_directory = r'C:\\Shared Files\\Udemy Files\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'{file_directory}people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<b'name'>, Column<b'name'>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name, df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StringType, StructType,\n",
    "                               IntegerType, StructField) # remember! brackets for new lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True),\n",
    "              StructField(\"name\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(age,LongType,true),StructField(name,StringType,true)))\n",
      "StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(df.schema)\n",
    "print(final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'{file_directory}people.json', schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema == final_struct, df.schema is final_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe basics part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|new_age|\n",
      "+----+-------+-------+\n",
      "|null|Michael|   null|\n",
      "|  30|   Andy|     30|\n",
      "|  19| Justin|     19|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_age', df.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = spark.createDataFrame([1, 2, 3]) # TypeError: Can not infer schema for type: <class 'int'> \"seriously?\"\n",
    "df2_fields = [StructField('age2', IntegerType(), False)]\n",
    "df2_schema = StructType(df2_fields)\n",
    "# the data arg 1 has to be passed as a list of tuples, or dicts, it can be a row object, or rdd or PD.DataFrame too\n",
    "df2 = spark.createDataFrame([(10,), (20,), (30,)], schema=df2_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age2|\n",
      "+----+\n",
      "|  10|\n",
      "|  20|\n",
      "|  30|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age2: int]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_2 = spark.createDataFrame([(10,), (20,), (30,)],\\\n",
    "                               schema=StructType([StructField('age2', IntegerType(), False)])) # bit longwinded\n",
    "df2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_3 = spark.createDataFrame([(10,), (20,), (30,)],\\\n",
    "                                 schema='age int') # much nicer can be long, short, double also \n",
    "df2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_row = pyspark.sql.Row(\"age2\") # for Row() if I just pass comma seperated stings those will be the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age2=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_row(1) # we can add a value like so but this does not seem like a good way to populate a df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok now were getting into it -REMEMBER!! range does not inlclude final number\n",
    "df3 = spark.createDataFrame([(i,) for i in range(1,4)], StructType([StructField(\"age3\", IntegerType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.age2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually this is because you can only withColumns an expression of the calling df or a lit() literal\n",
    "# df.withColumn(\"age2\", df2.age2) # this may error as the structField is not the same on nulls\n",
    "# df.withColumn('age3', df.age3).show() # nope cos this also errors...\n",
    "\n",
    "# this looks like it needs some kind of index thing - this is not Pandas or Numpy inplace magic...\n",
    "# https://stackoverflow.com/questions/42853778/add-a-column-from-another-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame([(i,) for i in range(1,4)], schema=StructType([StructField(\"age4\", IntegerType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.zip(df.rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.zip(df2.rdd).collect() # will error on Can only zip with RDD which has the same number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions(), df2.rdd.getNumPartitions(), df3.rdd.getNumPartitions(), df4.rdd.getNumPartitions()\n",
    "# why the heck do df2-4 have 8 partitions?\n",
    "# here - https://www.talend.com/blog/2018/03/05/intro-apache-spark-partitioning-need-know/ \n",
    "# it says There are many factors which affect partitioning choices like:\n",
    "#     Available resources — Number of cores on which task can run on.\n",
    "#     External data sources — Size of local collections, Cassandra table or HDFS file determine number of partitions.\n",
    "#  note this one   Transformations used to derive RDD \n",
    "#                 — There are a number of rules to determine the number of partitions \n",
    "#                 when a RDD is derived from another RDD.\n",
    "# so as we built the dfs in a different manner = different rdd partitons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-900f4ef7c666>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.rdd.repartition(8).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this gives horrific stack trace error poss cos repartition does not retun an rdd but a partition thingy\n",
    "# MapPartitionsRDD[194] at coalesce at NativeMethodAccessorImpl.java:0\n",
    "# df.rdd.repartition(8).zip(df2.rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = df.rdd.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.zip(df2.rdd).collect() # this errors on Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's flip that\n",
    "df.rdd.zip(df2.rdd.repartition(1)).collect()\n",
    "# this looks more interesting except the fist df is tuplized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trippy = schema is gone too\n",
    "df_col_add = spark.createDataFrame(df.rdd.zip(df2.rdd.repartition(1)).collect())\n",
    "df_col_add.show()\n",
    "## Perhaps you could do a withColumn on this split col _1 make a new col and replace the old one\n",
    "## and pass a new schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the world seems to think we should join...<br>\n",
    "this is an example of a trippy join with a complex condition from:<br>\n",
    "https://stackoverflow.com/questions/52850603/pyspark-add-columns-to-dataframe-based-on-values-from-different-dataframe<br>\n",
    "cond = (df_bb.cell == df_aa.cell1)|(df_bb.cell == df_aa.cell2)<br>\n",
    "df_bb.join(df_aa, cond, how=\"full\").withColumn(\"val1\", when((col(\"id1\")==col(\"id2\")) & ((col(\"cell\")==col(\"cell1\"))|(col(\"cell\")==col(\"cell2\"))) & (col(\"nr\")==1), 1).otherwise(0)).withColumn(\"val2\", when(~(col(\"id1\")==col(\"id2\")) & ((col(\"cell\")==col(\"cell1\"))|(col(\"cell\")==col(\"cell2\"))) & (col(\"nr\")==1), 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join needs an index\n",
    "# this don't seem to work AssertionError: col should be Column\n",
    "# this is the whole thingy here: from the Docstring\n",
    "#  The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
    "#  a column from some other :class:`DataFrame` will raise an error.\n",
    "df2.withColumn('name2', spark.sparkContext.parallelize([('Michael',),('Andy',),('Justin',)])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#et's look at the join syntax\n",
    "df_x_2 = df.join(df2) # this gives us cartesian/cross join...\n",
    "df_x_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_number() mod? 3\n",
    "# from pyspark.sql.functions import col, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "#df_x_2.select(df_x_2.columns).row_number().over(Window.orderBy(col(\"name\").desc()))\n",
    "# AttributeError: 'DataFrame' object has no attribute 'row_number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt 2\n",
    "# ref https://stackoverflow.com/questions/55690705/how-to-select-a-range-of-rows-from-a-dataframe-in-pyspark\n",
    "from pyspark.sql.functions import lit, row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# we can set up a window object with one partition and no order and apply it later - v. python\n",
    "w = Window().partitionBy(lit('a')).orderBy(lit('a'))\n",
    "w_nm = Window().orderBy('name') # I prefer this\n",
    "df_i = df.withColumn(\"row_num\", row_number().over(w_nm)) # I've heard you can use literals in a withColumn call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2_i = df2.withColumn(\"row_num\", row_number().over(w)) # here I still use the arbitrary as this is just demoing\n",
    "df2_i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# it's nice when it works\n",
    "df_j_1 = df_i.join(df2_i, on=\"row_num\")\n",
    "df_j_1.drop(\"row_num\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play time! this from the Databricks docs\n",
    "the Row() function looks like namedtuple<br>\n",
    "The rows can contain different lengthed sequences of rows<br>\n",
    "the resultant structure is much like a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n",
      "Row(department=Row(id='901234', name='Indoor Recreation'), employees=[Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000), Row(firstName='matei', lastName=None, email='no-reply@waterloo.edu', salary=140000)])\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)\n",
    "print(departmentWithEmployees4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44362f0a1bf21b8302c91c3241620b84716d9d06bdc4dec9c8d7bfb12114ae76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
