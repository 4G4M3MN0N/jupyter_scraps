{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\devenv\\\\SPARK\\\\spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This from the course\n",
    "spark = SparkSession.builder.appName('Ops').getOrCreate() # .builder not .Builder !! waht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is mashed together from the book + a medium article\n",
    "# If I run this after running the above it will error\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# findspark.find()\n",
    "# import pyspark\n",
    "# findspark.find()\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "# conf = SparkConf().setAppName('Ops').setMaster('local')\n",
    "# sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Shared Files\\Udemy Files\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Shared Files\\\\Udemy Files\\\\Python-and-Spark-for-Big-Data-master\\\\Spark_DataFrames'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = spark.read.json('people.json') # this errors - spark does not seem to care about chdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note need to escape the final \\ even in a raw string\n",
    "file_directory = r'C:\\Shared Files\\Udemy Files\\Python-and-Spark-for-Big-Data-master\\Spark_DataFrames\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'{file_directory}people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'name'>, Column<'name'>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name, df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StringType, StructType,\n",
    "                               IntegerType, StructField) # remember! brackets for new lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField(\"age\", IntegerType(), True),\n",
    "              StructField(\"name\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('age', LongType(), True), StructField('name', StringType(), True)])\n",
      "StructType([StructField('age', IntegerType(), True), StructField('name', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.schema)\n",
    "print(final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'{file_directory}people.json', schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema == final_struct, df.schema is final_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('age', IntegerType(), True), StructField('name', StringType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe basics part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|new_age|\n",
      "+----+-------+-------+\n",
      "|null|Michael|   null|\n",
      "|  30|   Andy|     30|\n",
      "|  19| Justin|     19|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_age', df.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = spark.createDataFrame([1, 2, 3]) # TypeError: Can not infer schema for type: <class 'int'> \"seriously?\"\n",
    "df2_fields = [StructField('age2', IntegerType(), False)]\n",
    "df2_schema = StructType(df2_fields)\n",
    "# the data arg 1 has to be passed as a list of tuples, or dicts, it can be a row object, or rdd or PD.DataFrame too\n",
    "df2 = spark.createDataFrame([(10,), (20,), (30,)], schema=df2_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age2|\n",
      "+----+\n",
      "|  10|\n",
      "|  20|\n",
      "|  30|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age2: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_2 = spark.createDataFrame([(10,), (20,), (30,)],\\\n",
    "                               schema=StructType([StructField('age2', IntegerType(), False)])) # bit longwinded\n",
    "df2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_3 = spark.createDataFrame([(10,), (20,), (30,)],\\\n",
    "                                 schema='age int') # much nicer can be long, short, double also \n",
    "df2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_row = pyspark.sql.Row(\"age2\") # for Row() if I just pass comma seperated stings those will be the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age2=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_row(1) # we can add a value like so but this does not seem like a good way to populate a df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok now were getting into it -REMEMBER!! range does not inlclude final number\n",
    "df3 = spark.createDataFrame([(i,) for i in range(1,4)], StructType([StructField(\"age3\", IntegerType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|age3|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|   3|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age2'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.age2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually this is because you can only withColumns an expression of the calling df or a lit() literal\n",
    "# df.withColumn(\"age2\", df2.age2) # this may error as the structField is not the same on nulls\n",
    "# df.withColumn('age3', df.age3).show() # nope cos this also errors...\n",
    "\n",
    "# this looks like it needs some kind of index thing - this is not Pandas or Numpy inplace magic...\n",
    "# https://stackoverflow.com/questions/42853778/add-a-column-from-another-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame([(i,) for i in range(1,4)], schema=StructType([StructField(\"age4\", IntegerType(), True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(age=None, name='Michael'), Row(age=None, name='Michael')),\n",
       " (Row(age=30, name='Andy'), Row(age=30, name='Andy')),\n",
       " (Row(age=19, name='Justin'), Row(age=19, name='Justin'))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.zip(df.rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rdd.zip(df2.rdd).collect() # will error on Can only zip with RDD which has the same number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, 8, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions(), df2.rdd.getNumPartitions(), df3.rdd.getNumPartitions(), df4.rdd.getNumPartitions()\n",
    "# why the heck do df2-4 have 8 partitions?\n",
    "# here - https://www.talend.com/blog/2018/03/05/intro-apache-spark-partitioning-need-know/ \n",
    "# it says There are many factors which affect partitioning choices like:\n",
    "#     Available resources — Number of cores on which task can run on.\n",
    "#     External data sources — Size of local collections, Cassandra table or HDFS file determine number of partitions.\n",
    "#  note this one   Transformations used to derive RDD \n",
    "#                 — There are a number of rules to determine the number of partitions \n",
    "#                 when a RDD is derived from another RDD.\n",
    "# so as we built the dfs in a different manner = different rdd partitons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.repartition(8).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this gives horrific stack trace error poss cos repartition does not retun an rdd but a partition thingy\n",
    "# MapPartitionsRDD[194] at coalesce at NativeMethodAccessorImpl.java:0\n",
    "# df.rdd.repartition(8).zip(df2.rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = df.rdd.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rdd.zip(df2.rdd).collect() # this errors on Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(age=None, name='Michael'), Row(age2=10)),\n",
       " (Row(age=30, name='Andy'), Row(age2=20)),\n",
       " (Row(age=19, name='Justin'), Row(age2=30))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's flip that\n",
    "df.rdd.zip(df2.rdd.repartition(1)).collect()\n",
    "# this looks more interesting except the fist df is tuplized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|             _1|  _2|\n",
      "+---------------+----+\n",
      "|{null, Michael}|{10}|\n",
      "|     {30, Andy}|{20}|\n",
      "|   {19, Justin}|{30}|\n",
      "+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trippy = schema is gone too\n",
    "df_col_add = spark.createDataFrame(df.rdd.zip(df2.rdd.repartition(1)).collect())\n",
    "df_col_add.show()\n",
    "## Perhaps you could do a withColumn on this split col _1 make a new col and replace the old one\n",
    "## and pass a new schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the world seems to think we should join...<br>\n",
    "this is an example of a trippy join with a complex condition from:<br>\n",
    "https://stackoverflow.com/questions/52850603/pyspark-add-columns-to-dataframe-based-on-values-from-different-dataframe<br>\n",
    "cond = (df_bb.cell == df_aa.cell1)|(df_bb.cell == df_aa.cell2)<br>\n",
    "df_bb.join(df_aa, cond, how=\"full\").withColumn(\"val1\", when((col(\"id1\")==col(\"id2\")) & ((col(\"cell\")==col(\"cell1\"))|(col(\"cell\")==col(\"cell2\"))) & (col(\"nr\")==1), 1).otherwise(0)).withColumn(\"val2\", when(~(col(\"id1\")==col(\"id2\")) & ((col(\"cell\")==col(\"cell1\"))|(col(\"cell\")==col(\"cell2\"))) & (col(\"nr\")==1), 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join needs an index\n",
    "# this don't seem to work AssertionError: col should be Column\n",
    "# this is the whole thingy here: from the Docstring\n",
    "#  The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
    "#  a column from some other :class:`DataFrame` will raise an error.\n",
    "# df2.withColumn('name2', spark.sparkContext.parallelize([('Michael',),('Andy',),('Justin',)])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "| age|   name|age2|\n",
      "+----+-------+----+\n",
      "|null|Michael|  10|\n",
      "|  30|   Andy|  10|\n",
      "|  19| Justin|  10|\n",
      "|null|Michael|  20|\n",
      "|  30|   Andy|  20|\n",
      "|  19| Justin|  20|\n",
      "|null|Michael|  30|\n",
      "|  30|   Andy|  30|\n",
      "|  19| Justin|  30|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#et's look at the join syntax\n",
    "df_x_2 = df.join(df2) # this gives us cartesian/cross join...\n",
    "df_x_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_number() mod? 3\n",
    "# from pyspark.sql.functions import col, row_number\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "#df_x_2.select(df_x_2.columns).row_number().over(Window.orderBy(col(\"name\").desc()))\n",
    "# AttributeError: 'DataFrame' object has no attribute 'row_number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt 2\n",
    "# ref https://stackoverflow.com/questions/55690705/how-to-select-a-range-of-rows-from-a-dataframe-in-pyspark\n",
    "from pyspark.sql.functions import lit, row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# we can set up a window object with one partition and no order and apply it later - v. python\n",
    "w = Window().partitionBy(lit('a')).orderBy(lit('a'))\n",
    "w_nm = Window().orderBy('name') # I prefer this\n",
    "df_i = df.withColumn(\"row_num\", row_number().over(w_nm)) # I've heard you can use literals in a withColumn call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------+\n",
      "| age|   name|row_num|\n",
      "+----+-------+-------+\n",
      "|  30|   Andy|      1|\n",
      "|  19| Justin|      2|\n",
      "|null|Michael|      3|\n",
      "+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|age2|row_num|\n",
      "+----+-------+\n",
      "|  10|      1|\n",
      "|  20|      2|\n",
      "|  30|      3|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_i = df2.withColumn(\"row_num\", row_number().over(w)) # here I still use the arbitrary as this is just demoing\n",
    "df2_i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on WindowSpec in module pyspark.sql.window object:\n",
      "\n",
      "class WindowSpec(builtins.object)\n",
      " |  WindowSpec(jspec: py4j.java_gateway.JavaObject) -> None\n",
      " |  \n",
      " |  A window specification that defines the partitioning, ordering,\n",
      " |  and frame boundaries.\n",
      " |  \n",
      " |  Use the static methods in :class:`Window` to create a :class:`WindowSpec`.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, jspec: py4j.java_gateway.JavaObject) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  orderBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      " |      Defines the ordering columns in a :class:`WindowSpec`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column` or list\n",
      " |          names of columns or expressions\n",
      " |  \n",
      " |  partitionBy(self, *cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')]]) -> 'WindowSpec'\n",
      " |      Defines the partitioning columns in a :class:`WindowSpec`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column` or list\n",
      " |          names of columns or expressions\n",
      " |  \n",
      " |  rangeBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      " |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative from the current row. For example,\n",
      " |      \"0\" means \"current row\", while \"-1\" means one off before the current row,\n",
      " |      and \"5\" means the five off after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          boundary start, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      end : int\n",
      " |          boundary end, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |  \n",
      " |  rowsBetween(self, start: int, end: int) -> 'WindowSpec'\n",
      " |      Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\n",
      " |      \n",
      " |      Both `start` and `end` are relative positions from the current row.\n",
      " |      For example, \"0\" means \"current row\", while \"-1\" means the row before\n",
      " |      the current row, and \"5\" means the fifth row after the current row.\n",
      " |      \n",
      " |      We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\n",
      " |      and ``Window.currentRow`` to specify special boundary values, rather than using integral\n",
      " |      values directly.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          boundary start, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedPreceding``, or\n",
      " |          any value less than or equal to max(-sys.maxsize, -9223372036854775808).\n",
      " |      end : int\n",
      " |          boundary end, inclusive.\n",
      " |          The frame is unbounded if this is ``Window.unboundedFollowing``, or\n",
      " |          any value greater than or equal to min(sys.maxsize, 9223372036854775807).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+\n",
      "| age|   name|age2|\n",
      "+----+-------+----+\n",
      "|  30|   Andy|  10|\n",
      "|  19| Justin|  20|\n",
      "|null|Michael|  30|\n",
      "+----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it's nice when it works\n",
    "df_j_1 = df_i.join(df2_i, on=\"row_num\")\n",
    "df_j_1.drop(\"row_num\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play time! this from the Databricks docs\n",
    "the Row() function looks like namedtuple<br>\n",
    "The rows can contain different lengthed sequences of rows<br>\n",
    "the resultant structure is much like a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id='123456', name='Computer Science')\n",
      "Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000)\n",
      "no-reply@berkeley.edu\n",
      "Row(department=Row(id='901234', name='Indoor Recreation'), employees=[Row(firstName='xiangrui', lastName='meng', email='no-reply@stanford.edu', salary=120000), Row(firstName='matei', lastName=None, email='no-reply@waterloo.edu', salary=140000)])\n"
     ]
    }
   ],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)\n",
    "print(departmentWithEmployees4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "165105423189813b728e25d2fd1994f7a3b630fd622bd243c8f5d69438ba1ffa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
