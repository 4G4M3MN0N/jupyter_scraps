{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# Import PySpark Pandas\n",
    "# import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\devenv\\\\SPARK\\\\spark-3.0.0-preview2-bin-hadoop2.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir(r'C:\\Users\\devenv\\SPARK\\spark-3.0.0-preview2-bin-hadoop2.7')\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this bit is in both the medium article and learning spark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this bit is just in the medium article\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "# see https://spark.apache.org/docs/latest/configuration.html#application-properties for app properties\n",
    "# app name is defined as The name of your application. This will appear in the UI and in log data. \n",
    "conf = SparkConf().setAppName('appName').setMaster('local')\n",
    "# setMaster(value) âˆ’ To set the master URL\n",
    "# Master could be \"local[2]\" and set up multple \"threads\" or look like .setMaster(\"spark://master:7077\")\n",
    "# you can also set the config blank and pass conf parameters at run time e.g. via the command line\n",
    "# this example is given\n",
    "# ./bin/spark-submit --name \"My app\" --master local[4] --conf spark.eventLog.enabled=false myApp.py\n",
    "\n",
    "# SparkConf() will load the values from spark.* Java system properties as well as what we pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the book\n",
    "sc = SparkContext(conf=conf)\n",
    "# this just medium\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc) # we seem to need this or we error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.map(lambda x: x * x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1 = nums.map(lambda x: [y for y in range(x * x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1, type(num1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb = nums.groupBy(lambda x : x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_object = num1.groupBy(lambda x : int(len(x)/10)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstGroup = group_object[0]\n",
    "[x for x in firstGroup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in firstGroup[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('C:\\Shared Files\\example_data_(002)\\Questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count(), lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylines = lines.filter(lambda x: 'leader' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylines, pylines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylines.first(), type(pylines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylines.first().split(',')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylines.take(1) # returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pylines.take(1)[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lines.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.map(lambda x: (x, 1)).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.map(lambda x: (x, 1)).take(5)[0],\\\n",
    "type(lines.map(lambda x: (x, 1)).take(5)[0]),\\\n",
    "type(lines.map(lambda x: (x, 1)).take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines.map(lambda x: (x, 1))), type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyspark.rdd.PipelinedRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines.foreach(lambda x : x + 'dude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lout = [x for x in range(lines.count())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "agg_tst = sc.parallelize([1, 2, 3, 4])\n",
    "agg_tst_2 = sc.parallelize([1, 2, 3, 4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tst.collect(), agg_tst_2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tst.glom().collect(), agg_tst_2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tst.aggregate((0, 0), seqOp, combOp)\n",
    "#      (10, 4)\n",
    "#  sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
    "#    (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lines.glom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tst.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_tst_2.collect(), agg_tst_2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize([1,2,3,4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspDf = spark.read.option(\"header\", True).csv(r\"C:\\Shared Files\\example_data_(002)\\ResponseData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rspDf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rspDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smmry = rspDf.summary().collect()\n",
    "smmry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(smmry) # hmm no headers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pass to the list to spark df \n",
    "smmrySDf = spark.createDataFrame(smmry)\n",
    "smmrySDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then pandas df\n",
    "smmryPDf = smmrySDf.toPandas()\n",
    "smmryPDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rspDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rspDf.stat.freqItems(['Response'], support=.99).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rspDf.Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspDf.Response.contains('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rspDf.select(rspDf.Response.contains('1')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rspDf_int_resp = rspDf.withColumn('Response', rspDf.Response.cast('int'))\n",
    "rspDf_int_resp.groupby('QuestionID').sum('Response').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rspDfgp =(\n",
    "rspDf  # rspDf_int_resp\n",
    ".groupby(['QuestionID','ResponseID'])\n",
    ".agg({'Response':'count'})  # .count()  # .max('Response')\n",
    ".sort('QuestionID')\n",
    ".filter(rspDf.QuestionID.isin(['1014663','1014649']))\n",
    ") \n",
    "rspDfgp.show(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rspDfgp.filter(rspDfgp['count(Response)'] > 1).select('ResponseID').show()  #.select(rspDfgp.ResponseID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qrDf = rspDf.groupby(['QuestionID', 'Response']).count() # get a pivot of question ids and responses\n",
    "qrDf.sort('count', ascending=False).show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe simpler in sql\n",
    "try:\n",
    "    rspDf.createTempView(\"rspsql\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "'''select QuestionID,\n",
    "ResponseID,\n",
    "count(Response)\n",
    "from rspsql\n",
    "where QuestionID\n",
    "in (\\'1014663\\',\\'1014649\\')\n",
    "group by QuestionID, ResponseID\n",
    "having count(Response) >1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can do correlared subqueries\n",
    "spark.sql(\n",
    "    '''select distinct QuestionID,\n",
    "    (select max(a.Response) from rspsql a where a.QuestionID = b.QuestionID and response <> 5)\n",
    "    from rspsql b''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a useable pyspark dataframe\n",
    "type(spark.sql(\n",
    "    '''select distinct QuestionID,\n",
    "    (select max(a.Response) from rspsql a where a.QuestionID = b.QuestionID and response <> 5)\n",
    "    from rspsql b''')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top5qrDf = qrDf.groupby(['QuestionID']).agg(sf.sum('count').alias('nrResponses')) \\\n",
    "                                            .sort('nrResponses', ascending=False).take(5)\n",
    "print(top5qrDf, '\\n')\n",
    "top5QLs = [x.QuestionID for x in top5qrDf]\n",
    "print(top5QLs, '\\n')\n",
    "top5Qs = qrDf.filter(qrDf.QuestionID.isin(top5QLs)).sort('QuestionID')\n",
    "top5Qs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about we add a column to top5Qs to show the total responses for that question\n",
    "QuestionResponses = qrDf.groupby(['QuestionID']).agg(sf.sum('count').alias('nrResponses')) \\\n",
    "                                        .sort('nrResponses', ascending=False)\n",
    "print(QuestionResponses.head(2),'\\n')\n",
    "thig = top5Qs.join(QuestionResponses, on='QuestionID', how='inner')\n",
    "thig.show(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rspDf.rdd.getNumPartitions())\n",
    "rep_rspDf = rspDf.repartition(8)\n",
    "rspDf.rdd.getNumPartitions(), rep_rspDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitons & mapPartitionsWithIndex - pass the partition to the function with values as an iteraror.\n",
    "# This is more efficient as instances (i.e functions) only have to be created once per partition not for each element\n",
    "\n",
    "tst = sc.parallelize([[1, 2], [2, 3], [3, 4], [4, 5]], 4)\n",
    "def f(ix, val): yield (ix, type(val))\n",
    "tst.mapPartitionsWithIndex(f).collect()  # WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): yield (ix, [x for x in val][0])\n",
    "tst.mapPartitionsWithIndex(f).collect()  # WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): return (ix, type(val))\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): return (ix, [x for x  in val][0])\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): return (ix, *val)\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): yield (ix, *val)\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): yield [x for x in val][0]\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tst = sc.parallelize([[1,2], [2,3], [3,4], [4,5]], 4)\n",
    "def f(ix, val): return [x for x in val][0]\n",
    "tst.mapPartitionsWithIndex(f).collect() #WithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "dfNms = spark.createDataFrame(data=arrayData, schema=['name','knownLanguages','properties'])\n",
    "dfNms.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "dfNms2 = dfNms.select(dfNms.name, explode(dfNms.knownLanguages).alias('knownLanguages'), dfNms.properties)\n",
    "dfNms2.printSchema()\n",
    "dfNms2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNms3 = dfNms2.select(dfNms2.name, dfNms2.knownLanguages, explode(dfNms2.properties))\n",
    "dfNms3.printSchema()\n",
    "dfNms3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Testing out Pyspark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respdf1 = spark.read.option(\"header\", True).csv(r\"C:\\Shared Files\\example_data_(002)\\ResponseData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(respDf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respdf1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf1 = spark.read.option(\"header\", True).csv('C:\\Shared Files\\example_data_(002)\\Questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(qdf1.count(), len(qdf1.columns)) # gets us the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf1.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "165105423189813b728e25d2fd1994f7a3b630fd622bd243c8f5d69438ba1ffa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
